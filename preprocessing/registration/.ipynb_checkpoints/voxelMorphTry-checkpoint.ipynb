{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://colab.research.google.com/drive/1SkQbrWTQHpQFrG4J2WoBgGZC9yAzUas2?usp=sharing#scrollTo=Z1lwekV6pLYz\n",
    "\n",
    "# some imports we'll need throughout the demo\n",
    "import os\n",
    "\n",
    "# some third party very useful libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa  # for TQDM callback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import nibabel as nib\n",
    "\n",
    "# our libraries\n",
    "import voxelmorph as vxm\n",
    "import neurite as ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off eager for this\n",
    "# need to do it due to some tf 2.0+ compatibility issues\n",
    "tf.compat.v1.disable_eager_execution()",
    "tf.compat.v1.disable_eager_execution()",
    "tqdm_cb = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False, show_epoch_progress=False) ",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpful functions\n",
    "def plot_hist(hist):\n",
    "  plt.figure(figsize=(17,5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(hist.epoch, hist.history['loss'], '.-')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epochs');\n",
    "  plt.subplot(1, 2, 2)\n",
    "  nb_epochs = len(hist.epoch) // 2\n",
    "  plt.plot(hist.epoch[-nb_epochs:], hist.history['loss'][-nb_epochs:], '.-')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epochs');\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load up MNIST\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "x_train_all = x_train_all.astype('float')/255\n",
    "x_test_all = x_test_all.astype('float')/255\n",
    "\n",
    "x_train_all = np.pad(x_train_all, ((0, 0), (2, 2), (2, 2)), 'constant')[..., np.newaxis]\n",
    "x_test_all = np.pad(x_test_all, ((0, 0), (2, 2), (2, 2)), 'constant')[..., np.newaxis]\n",
    "\n",
    "vol_shape = list(x_train_all.shape[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all 3s\n",
    "digit = 3\n",
    "\n",
    "x_train = x_train_all[y_train_all == digit, ...]\n",
    "y_train = y_train_all[y_train_all == digit]\n",
    "x_test = x_test_all[y_test_all == digit, ...].astype('float')/255\n",
    "y_test = y_test_all[y_test_all == digit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a simple generator. \n",
    "def template_gen(x, batch_size):\n",
    "  vol_shape = list(x.shape[1:-1])\n",
    "  zero = np.zeros([batch_size] + vol_shape + [2])\n",
    "  mean_atlas = np.repeat(  np.mean(x, 0, keepdims=True), batch_size, 0)\n",
    "\n",
    "  while True:\n",
    "    idx = np.random.randint(0, x.shape[0], batch_size)\n",
    "    img = x[idx, ...]\n",
    "    inputs = [mean_atlas, img]\n",
    "    outputs = [img, zero, zero, zero]\n",
    "    yield inputs, outputs\n",
    "\n",
    "# let's make sure the sizes make sense\n",
    "sample = next(template_gen(x_train, 8))\n",
    "[f.shape for f in sample[0]], [f.shape for f in sample[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to MNIST, all digits this time\n",
    "x_train = x_train_all\n",
    "y_train = y_train_all\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train_all, 10)\n",
    "x_test = x_test_all\n",
    "y_test = y_train_all\n",
    "vol_shape = list(x_train.shape[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a simple generator. \n",
    "def cond_template_gen(x, y, batch_size):\n",
    "  vol_shape = list(x.shape[1:-1])\n",
    "  zero = np.zeros([batch_size] + vol_shape + [2])\n",
    "  atlas = np.repeat(np.mean(x, 0, keepdims=True), batch_size, 0)\n",
    "\n",
    "  while True:\n",
    "    idx = np.random.randint(0, x.shape[0], batch_size)\n",
    "    img = x[idx, ...]\n",
    "    inputs = [y[idx, ...], atlas, img]\n",
    "\n",
    "    outputs = [img, zero, zero, zero]\n",
    "    yield inputs, outputs\n",
    "\n",
    "sample = next(cond_template_gen(x_train, y_train_onehot, 8))\n",
    "[f.shape for f in sample[0]], [f.shape for f in sample[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_enc = [16,32,32,32]\n",
    "nf_dec = [32,32,32,32,16,16,3] \n",
    "enc_nf = [16, 32, 32, 32]",
    "dec_nf = [32, 32, 32, 32, 32, 16, 16]",
    "model = vxm.networks.ConditionalTemplateCreation(vol_shape, pheno_input_shape=[10], nb_unet_features=[enc_nf, dec_nf], conv_nb_features=16,\n",
    "                                                 conv_image_shape = [4, 4, 8], conv_nb_levels=4)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare losses\n",
    "image_loss_func = vxm.losses.MSE().loss\n",
    "losses = [image_loss_func, vxm.losses.MSE().loss, vxm.losses.Grad('l2', loss_mult=2).loss, vxm.losses.MSE().loss]\n",
    "loss_weights = [1, 0.01, 0.03, 0]  # changed second-last to 0.01\n",
    "\n",
    "\n",
    "model.compile('adam', loss=losses, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "gen = cond_template_gen(x_train, y_train_onehot, batch_size=32)\n",
    "hist = model.fit(gen, epochs=100, steps_per_epoch=25, verbose=0, callbacks=[tqdm_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_model = tf.keras.models.Model(model.inputs[:2], model.get_layer('atlas').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_atlas = np.repeat(np.mean(x_train, 0, keepdims=True), 10, 0)\n",
    "input_samples = [tf.keras.utils.to_categorical(np.arange(10), 10), mean_atlas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = atlas_model.predict(input_samples)\n",
    "ne.plot.slices([f.squeeze() for f in pred], cmaps=['gray']);"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
